{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab59653c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ac2bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b13599d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e67029",
   "metadata": {},
   "source": [
    "# 1. Data Understanding\n",
    "Familiarize yourself with the dataset and interpret the significance of each feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86309559",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "041cb2f7",
   "metadata": {},
   "source": [
    "From the statistics above, we can understand that the variables can have been scaled.  \n",
    "But the idea of Normalization is far from being used:  \n",
    "\n",
    "Gaussian distribution normalization is excluded due to the following reasons:\n",
    "- The means of the variables are not 0.\n",
    "- The standard deviations of the variables are not 1.\n",
    "\n",
    "Min-Max normalization is excluded for the following reasons:\n",
    "\n",
    "- The minimum values of the variables are not 0.\n",
    "- The maximum values of the variables are not 1.\n",
    "\n",
    "The use of Principal Component Analysis is far from being used also, as it contains columns not on the same scale, like 'col_10' witch use range of values more higher than others"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf17107",
   "metadata": {},
   "source": [
    "Let's see how many value in each feature, to understand what they can be "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca857cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df.columns:\n",
    "    print(f\"Count of distinct values for {col}:\",len(df[col].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b47c24a7",
   "metadata": {},
   "source": [
    "We can understand that \"col_1\", \"col_3\" and the target \"label\"  can be categorical variables, their values respectively [0,1], [0,0.5,1] and [0,1].  \n",
    "And all the other variables are numerical.  \n",
    "But at the moment that all of them are technicaly numerical, they will be considered like that for the modeling part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcbcf7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb8dd9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMERICAL = ['col_0', 'col_2', 'col_4', 'col_5', 'col_6', 'col_7', 'col_8', 'col_9', 'col_10', 'col_11', 'col_12', 'col_13']\n",
    "CATEGORICAL = ['col_1', 'col_3']\n",
    "TARGET = ['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dfeaa1b",
   "metadata": {},
   "source": [
    "#### Check the Normal Distribution  \n",
    "I was about to use Shapiro test, but as the number of values is over 5000, this test will not work.  \n",
    "In this casel, let's use Anderson test.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf70b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scipy.stats import shapiro\n",
    "\n",
    "# for column in df.columns:\n",
    "#     stat, p = shapiro(df[column])\n",
    "#     alpha = 0.05\n",
    "#     if p > alpha:\n",
    "#         print(f'Column {column} looks Gaussian (fail to reject H0)')\n",
    "#     else:\n",
    "#         print(f'Column {column} does not look Gaussian (reject H0)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f641496a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import anderson\n",
    "\n",
    "for column in df.columns:\n",
    "    result = anderson(df[column])\n",
    "    print(f\"\\n{column}:\")\n",
    "\n",
    "    for i in range(len(result.critical_values)):\n",
    "        sl, cv = result.significance_level[i], result.critical_values[i]\n",
    "        if result.statistic < cv:\n",
    "            print(f'At {sl}% significance level, data looks normal (H1)')\n",
    "        else:\n",
    "            print(f'At {sl}% significance level, data does not look normal (H0)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "600ee884",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f92647",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count plot of categorical variables\n",
    "for feature in CATEGORICAL:\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.countplot(data=df, x=feature)\n",
    "    plt.title(f'Count Plot of {feature}')\n",
    "    plt.xlabel(feature)\n",
    "    plt.ylabel('Count')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "668845bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in NUMERICAL:\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.histplot(data=df, x=feature, kde=True)\n",
    "    plt.title(f'Histogram of {feature}')\n",
    "    plt.xlabel(feature)\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2d4408",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box plots of numerical variables\n",
    "for feature in NUMERICAL:\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.boxplot(data=df, y=feature)\n",
    "    plt.title(f'Box Plot of {feature}')\n",
    "    plt.ylabel(feature)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c1f57f",
   "metadata": {},
   "source": [
    "# 2. Feature Importance\n",
    "Identify the essential features in the dataset. What makes these features significant? How do they influence the outcome variable?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc72f26",
   "metadata": {},
   "source": [
    "#### a) Analyse the corrolation  \n",
    "To determine the influence of each variable on the outcome variable, it is necessary to analyze the correlation with the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae2350c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Compute the correlation matrix\n",
    "corr_matrix = df.corr()\n",
    "\n",
    "# Sort the correlation matrix based on the target variable\n",
    "target_correlation = corr_matrix['label'].sort_values(ascending=False)\n",
    "\n",
    "# Plotting the correlation matrix as a heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2220a882",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the correlation of features to the target variable\n",
    "plt.figure(figsize=(8, 6))\n",
    "target_correlation.plot(kind='bar')\n",
    "plt.title('Correlation of Features to label')\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Correlation')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2801c862",
   "metadata": {},
   "source": [
    "To make an analysis about which one is \"Highly correlated\", we can chose a threshold of 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256177c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "correlations = corr_matrix['label']\n",
    "high_corr_features = correlations[correlations.abs() > 0.5]\n",
    "print(\"Features highly correlated with target variable:\", high_corr_features.index.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7277a764",
   "metadata": {},
   "source": [
    "We can observe that the contribution of all variables is low."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ecea27b",
   "metadata": {},
   "source": [
    "#### b) Mutual information scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5030b35",
   "metadata": {},
   "source": [
    "For further analysis, we will assess whether the relationship between variables is nonlinear and examine the contribution of each variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27035f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "\n",
    "# Separate the features (X) and the target (y)\n",
    "X = df.drop('label', axis=1)\n",
    "y = df['label']\n",
    "\n",
    "# Calculate mutual information\n",
    "mi = mutual_info_classif(X, y)\n",
    "\n",
    "# Convert mutual information scores to a DataFrame\n",
    "mi_series = pd.Series(mi, index=X.columns)\n",
    "\n",
    "# Print the features sorted by mutual information score\n",
    "print(mi_series.sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f2106e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(mi_series.sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f0350d7",
   "metadata": {},
   "source": [
    "All the variables have a mutual information score close to 0, indicating a low contribution even with nonlinear analysis. However, we can identify the top 6 features that have a score higher than 0.05."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be179a09",
   "metadata": {},
   "source": [
    "#### c) Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381699ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "X = df.drop('label', axis=1)\n",
    "y = df['label']\n",
    "\n",
    "model = RandomForestClassifier()\n",
    "model.fit(X, y)\n",
    "\n",
    "importances = model.feature_importances_\n",
    "for feature, importance in zip(X.columns, importances):\n",
    "    print(f\"Feature: {feature}, Importance: {importance}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a465ee60",
   "metadata": {},
   "source": [
    "#### d) L1 regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc6220a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "lasso = Lasso(alpha=0.004)\n",
    "\n",
    "lasso.fit(X, y)\n",
    "\n",
    "importance = np.abs(lasso.coef_)\n",
    "\n",
    "feature_importance = sorted(list(zip(X.columns, importance)), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "for feature, importance in feature_importance:\n",
    "    print(f\"Feature: {feature}, Importance: {importance}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce49093f",
   "metadata": {},
   "source": [
    "Based on the L1 regularization, alpha = 0.004,  the important features are : ['col_1','col_3','col_6','col_9','col_10','col_11','col_12']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185906de",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMPORTANT_FEATURES = ['col_1','col_3','col_6','col_9','col_10','col_11','col_12']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f78b30d8",
   "metadata": {},
   "source": [
    "The features: 'col_12', 'col_6' are mentioned in all methods, that give them more importance, even if they are with low contribution overall"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e71206",
   "metadata": {},
   "source": [
    "# 3. Outlier Detection\n",
    "Can you identify outliers in the dataset? What techniques do you use to identify these outliers, and how would you handle them?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c005eeb",
   "metadata": {},
   "source": [
    "Many techniques can be used to handle outliers, and here I will detail:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7719b243",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('label', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "231a87d1",
   "metadata": {},
   "source": [
    "### 1. Gaussian distribution (or Zscore)\n",
    "Before using this method, we have to check if our variables follow the gaussian distribution, but as mentioned in section 1.a) No one of our variables is normally distributed, so we cannot use it. Except if we suppose that the data is normally distributed, and any other values are considered outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f17951",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import zscore\n",
    "\n",
    "z_scores = zscore(df)\n",
    "\n",
    "mask = np.abs(z_scores) > 3\n",
    "\n",
    "outliers = df.where(mask)  # This will replace non-outliers with NaNs\n",
    "outliers = outliers.dropna(how='all')\n",
    "print(f\"Number of outliers using Zscore: {len(outliers)} that in percentage {round(100*len(outliers)/len(df),2)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62fb807a",
   "metadata": {},
   "source": [
    "As the variables are not normally distributed, this method of outliers detection can give use wrong result, unless we support that they have to be normal.  \n",
    "We will change the threshold to 4, to minimise the lose of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38afe4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = np.abs(z_scores) > 4\n",
    "outliers = df.where(mask)\n",
    "outliers = outliers.dropna(how='all')\n",
    "print(f\"Number of outliers using Zscore: {len(outliers)} that in percentage {round(100*len(outliers)/len(df),2)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1795c64",
   "metadata": {},
   "source": [
    "The percentage is more acceptable even if it's high, but we will adopt it for the rest of project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9234ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = np.abs(z_scores) < 4\n",
    "df = df.where(mask)\n",
    "df = df.dropna(how='any')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b3cb4f",
   "metadata": {},
   "source": [
    "#### 2. IQR (Interquartile Range) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d330b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1 = X.quantile(0.25)\n",
    "Q3 = X.quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# Define a mask for values outside the IQR range\n",
    "mask = ((X < (Q1 - 1.5 * IQR)) | (X > (Q3 + 1.5 * IQR)))\n",
    "\n",
    "outliers = X.where(mask)  # This will replace non-outliers with NaNs\n",
    "outliers = outliers.dropna(how='all')\n",
    "print(f\"Number of outliers using IQR: {len(outliers)} that in percentage {round(100*len(outliers)/len(df),2)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9bed60",
   "metadata": {},
   "source": [
    "The IQR method yielded a wide range of \"outliers,\" but the concern of potentially losing significant data led me to explore alternative methods for more thorough analysis and informed decision-making."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c047618c",
   "metadata": {},
   "source": [
    "#### 3. Histograms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b9af1db",
   "metadata": {},
   "source": [
    "This method is based on histograms and the assumption of a normal distribution. The further the distribution deviates from normal, the more likely it is to be detected as outliers. While this method can be useful under the assumption of a normal data distribution, it may not be as effective when the distribution deviates significantly from normal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da451ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "threshold = 3\n",
    "\n",
    "# Detect outliers using histograms\n",
    "outliers = pd.DataFrame()\n",
    "for column in df.columns:\n",
    "    feature = df[column]\n",
    "    mean = feature.mean()\n",
    "    std = feature.std()\n",
    "    cutoff = mean + threshold * std\n",
    "    outliers = outliers.append(df[feature > cutoff])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a0ffd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot histograms with outliers highlighted\n",
    "for column in df.columns:\n",
    "    feature = df[column]\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.hist(feature, bins='auto', alpha=0.7, color='blue')\n",
    "    plt.hist(outliers[column], bins='auto', alpha=0.5, color='red')\n",
    "    plt.xlabel(column)\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title(f'Histogram of {column}')\n",
    "    plt.legend(['Data', 'Outliers'])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456a363b",
   "metadata": {},
   "source": [
    "# 4. Feature Engineering\n",
    "How would you create new features from the existing ones to better capture the underlying patterns in the data?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9659b1e",
   "metadata": {},
   "source": [
    "The idea is to create some combinations between features, and here we can use 2 methods:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f251a233",
   "metadata": {},
   "source": [
    "#### a) Polynomial features  \n",
    "The idea is to multiply each feature by itself and observe if this transformation enhances the capture of underlying patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554de218",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "poly = PolynomialFeatures(degree=2)\n",
    "X_poly = poly.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2af35db",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_poly.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437a353c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Compute the correlation matrix\n",
    "corr_matrix = pd.DataFrame(np.c_[X_poly, df['label']]).corr()\n",
    "\n",
    "# Sort the correlation matrix based on the target variable\n",
    "correlations = corr_matrix[corr_matrix.columns[-1]]\n",
    "high_corr_features = correlations[correlations.abs() > 0.5]\n",
    "print(\"Features highly correlated with target variable:\", high_corr_features.index.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de49ccd8",
   "metadata": {},
   "source": [
    "#### b) Interaction features \n",
    "The idea is to create a cross multiplication of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4fa5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "from sklearn.linear_model import Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e76afb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "important_features = set()\n",
    "seen= set()\n",
    "X = df.drop('label', axis=1)\n",
    "X_ = df.drop('label', axis=1)\n",
    "\n",
    "y = df['label']\n",
    "for i in tqdm(range(len(df.columns)-1)):\n",
    "    X = df.drop('label', axis=1)\n",
    "    for j in range(len(df.columns)-1):\n",
    "        if j>=i:\n",
    "            col1 = df.columns[i]\n",
    "            col2 = df.columns[j]\n",
    "            \n",
    "            if f'{col1}_{col2}' not in seen:\n",
    "                X[f'{col1}_{col2}'] = X[col1] * X[col2]\n",
    "                seen.add(f'{col1}_{col2}')\n",
    "\n",
    "    lasso = Lasso(alpha=1)\n",
    "\n",
    "    lasso.fit(X, y)\n",
    "\n",
    "    importance = np.abs(lasso.coef_)\n",
    "\n",
    "    feature_importance = sorted(list(zip(X.columns, importance)), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    for feature, importance in feature_importance:\n",
    "        if importance >0:\n",
    "            print(f\"Feature: {feature}, Importance: {importance}\")\n",
    "            X_[feature] = X[feature]\n",
    "            important_features.add(f'{feature}')\n",
    "print(len(important_features))\n",
    "print(important_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "321ef0b0",
   "metadata": {},
   "source": [
    "Despite the low scores observed for some cross-features, testing them did not result in significant improvements. Consequently, these features will not be included or adopted."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6963522",
   "metadata": {},
   "source": [
    "# 5. Model Building and Evaluation\n",
    "Build a predictive model using the dataset. Which model did you choose and why? How well does your model perform?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d397be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('label', axis=1)\n",
    "# Use only important features\n",
    "# X = X[IMPORTANT_FEATURES]\n",
    "y = df['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad0b9cf",
   "metadata": {},
   "source": [
    "#### a) Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c6123d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Initialize DecisionTreeClassifier\n",
    "dt = DecisionTreeClassifier()\n",
    "\n",
    "# Fit the model\n",
    "dt.fit(X_train, y_train)\n",
    "\n",
    "# Predict the response for test dataset\n",
    "y_pred = dt.predict(X_test)\n",
    "\n",
    "# Model Accuracy\n",
    "print(\"Decision Tree model accuracy(in %):\", accuracy_score(y_test, y_pred)*100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b544573",
   "metadata": {},
   "source": [
    "#### b) Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c379ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc9b26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ad0ad4",
   "metadata": {},
   "source": [
    "For the time that all the variables are numerical, we will consider that like that for the modeling part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89604912",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMERIC_FEATURE = df.columns[:-1]\n",
    "# NUMERIC_FEATURE = ['col_0', 'col_1', 'col_2', 'col_3', 'col_4', 'col_6', 'col_7', 'col_9', 'col_10', 'col_11', 'col_12', 'col_13']\n",
    "# NUMERIC_FEATURE = IMPORTANT_FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7956391a",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_columns = []\n",
    "for feature in NUMERIC_FEATURE:\n",
    "    num_col = tf.feature_column.numeric_column(feature)\n",
    "    numeric_columns.append(num_col)\n",
    "feature_columns = numeric_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aade153",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d93df76",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(df, test_size=0.2)\n",
    "print(len(train), 'train examples')\n",
    "print(len(test), 'test examples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99aa4764",
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_dataset(dataframe,shuffle=True, batch_size=32):\n",
    "    dataframe = dataframe.copy()\n",
    "    \n",
    "    labels = dataframe.pop('label')\n",
    "    \n",
    "    features = dataframe\n",
    "    \n",
    "    ds = tf.data.Dataset.from_tensor_slices((dict(features),labels.values))\n",
    "    \n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(buffer_size=len(dataframe))\n",
    "    \n",
    "    ds = ds.batch(batch_size)\n",
    "    \n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ebee333",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1024\n",
    "ds_train = df_to_dataset(train, batch_size=batch_size)\n",
    "ds_test = df_to_dataset(test, shuffle=False, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd03eef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.DenseFeatures(feature_columns),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dense(16, activation='relu'),\n",
    "    tf.keras.layers.Dense(16, activation='relu'),\n",
    "    tf.keras.layers.Dense(8, activation='relu'),\n",
    "    tf.keras.layers.Dense(4, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e361ee70",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "             loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3baad3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(ds_train,\n",
    "                    validation_data=ds_test,\n",
    "                    epochs=200)\n",
    "# loss: 0.5379 - accuracy: 0.7210 - val_loss: 0.5376 - val_accuracy: 0.7209"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d08a084",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy = model.evaluate(ds_test)\n",
    "# loss: 0.5377 - accuracy: 0.7204"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa72a869",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_accuracy = history.history['accuracy']\n",
    "train_loss = history.history['loss']\n",
    "val_accuracy = history.history['val_accuracy']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(len(train_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7208b5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(epochs,train_accuracy,label='train_accuracy')\n",
    "plt.plot(epochs,val_accuracy,label='val_accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739b2db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(epochs,train_loss,label='train_loss')\n",
    "plt.plot(epochs,val_loss,label='val_loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c416ee34",
   "metadata": {},
   "source": [
    "The graphs effectively illustrate how well the model fits and converges to the achievable accuracy.  \n",
    "After conducting all the tests, I have decided to adopt the FNN model with an accuracy of 72%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce221e7",
   "metadata": {},
   "source": [
    "# 6. Communication\n",
    "Describe your process and findings clearly and understandably. Are you able to simplify complex data science concepts into everyday language?    \n",
    "#### a) Describe your process and findings clearly and understandably\n",
    "\n",
    "The dataset provided posed a significant challenge, but my curiosity drove me to explore and comprehend it. However, due to time constraints, I had to allocate my time across different questions, leaving room for further exploration in each area.  \n",
    "\n",
    "I began by thoroughly understanding the data since it was crucial to have a clear understanding of the dataset at hand. I utilized various statistical descriptive methods and tests to explore the distribution of the dataset. Visualizations played a vital role in gaining different perspectives on the data.  \n",
    "\n",
    "Determining the feature importance was a critical step, as it required conducting multiple statistical tests and employing various methods to quantify each feature's contribution to the target variable. This process allowed me to identify the most significant feature that influenced the knowledge of the target.  \n",
    "\n",
    "Detecting outliers proved to be a challenging task as it necessitated identifying and handling outliers without sacrificing valuable information they might contain. Features that contained outliers had the potential to impact the entire dataset, unless we implemented methods to replace outliers with interpolated data or filled them with averages. This process often involved dealing with missing values.  \n",
    "\n",
    "Feature engineering played a vital role in enhancing the dataset's power and ensuring a better representation of the target variable. This step involved creating new features that might have been hidden in the original dataset.  \n",
    "\n",
    "The modeling phase, while comparatively straightforward, required sensitivity. Choosing the appropriate models, fine-tuning hyperparameters, and analyzing model logs could be time-consuming but necessary to achieve the objective of achieving higher accuracy.\n",
    "\n",
    "\n",
    "#### b) Are you able to simplify complex data science concepts into everyday language?\n",
    "The field of Data Science continues to evolve, becoming increasingly complex over time, especially with the integration of Artificial Intelligence which remains a research domain. New concepts emerge regularly, adding to the challenge of simplifying and conveying these ideas effectively. However, at its core, Data Science is no more than an advanced application of everyday life experiences, enhanced by technology. Leveraging relatable analogies and real-life examples proves to be an effective strategy in simplifying complex concepts. Additionally, incorporating storytelling techniques establishes a connection with the audience, fostering better understanding and enabling them to immerse themselves in the intricacies of the concept, thus enhancing memory retention. Simplifying concepts through non-technical graphs aids in visual comprehension, allowing the audience to grasp and absorb information more efficiently compared to solely auditory explanations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
